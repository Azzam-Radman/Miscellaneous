You are certainly already aware, but just in case, if you use TF KERAS with a kernel using GPU cards (the latest), you could speed up your workout by around 30% with a few simple lines.
With my laptop, the lines below reduce an epoch duration from 30 to 20 seconds. Cool !

from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)

TF recommands to change the final layer type when it is softmax (maybe sigmoid as well) to float32 for more stability :
output_layer = tf.keras.layers.Activation('sigmoid', dtype='float32')(output_layer)
